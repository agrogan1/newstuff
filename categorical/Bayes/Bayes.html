<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<style>
/* CSS for Markstat 2.0 using Pandoc 2.0 */
body{padding:14px 28px;}
body, table {font-family: Helvetica, Arial, Sans-serif; font-size: 14px;}
h1, h2, h3, h4 {font-weight: normal; color: #3366cc}
h1 {font-size: 200%;}
h2 {font-size: 150%;}
h3 {font-size: 120%;}
h4 {font-size: 100%; font-weight:bold}
img.center {display:block; margin-left:auto; margin-right:auto}
.small{font-size:8pt;}
a {color: black;}
a:visited {color: #808080;}
a.plain {text-decoration:none;}
a.plain:hover {text-decoration:underline;}
.em {font-weight:bold;}
pre, code {font-family: "lucida console", monospace;}
pre.stata {font-size:13px; line-height:13px;}
pre {padding:8px; border:1px solid #c0c0c0; border-radius:8px; background-color:#fdfdfd;}
code {color:#3366cc; background-color:#fafafa;}
pre code { color:black; background-color:white}
/* Added for Pandoc */
figure > img, div.figure > img {display:block; margin:auto}
figcaption, p.caption {text-align:center; font-weight:bold; color:#3366cc;}
h1.title {text-align:center; margin-bottom:0}
p.author, h2.author {font-style:italic; text-align:center;margin-top:4px;margin-bottom:0}
p.date, h3.date {text-align:center;margin-top:4px; margin-bottom:0}
/* Tables*/
table { margin:auto; border-collapse:collapse; }
table caption { margin-bottom:1ex;}
th, td { padding:4px 6px;}
thead tr:first-child th {border-top:1px solid black; padding-top:6px}
thead tr:last-child  th {padding-bottom:6px}
tbody tr:first-child td {border-top:1px solid black; padding-top:6px}
tbody tr:last-child  td {padding-bottom:6px;}
table tbody:last-child tr:last-child td {border-bottom:1px solid black;}
</style>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Andy Grogan-Kaylor" />
  <title>Bayesian Categorical Data Analysis</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Bayesian Categorical Data Analysis</h1>
<p class="author">Andy Grogan-Kaylor</p>
<p class="date">27 Nov 2020 11:02:13</p>
</header>
<h1 id="introduction">Introduction</h1>
<style>h1 {color: #00274C;} h2 {color: #2F65A7;} blockquote {border-left: 5px solid #ffcb05; margin: 1.5em 10px; padding: 0.5em 10px;} code {color: red;}</style>
<pre class='stata'>. clear all
</pre>
<h1 id="the-importance-of-thinking-about-prior-information">The Importance of Thinking About Prior Information</h1>
<blockquote>
<p><a href="https://agrogan.shinyapps.io/Thinking-Through-Bayes/">Thinking Through Bayesian Ideas</a></p>
</blockquote>
<h1 id="more-about-priors-from-sas-corporation">More About Priors From SAS Corporation</h1>
<blockquote>
<p>“In addition to data, analysts often have at their disposal useful auxiliary information about inputs into their model—for example, knowledge that high prices typically decrease demand or that sunny weather increases outdoor mall foot traffic. If used and incorporated correctly into the analysis, the auxiliary information can significantly improve the quality of the analysis. But this information is often ignored. Bayesian analysis provides a principled means of incorporating this information into the model through the prior distribution, but it does not provide a road map for translating auxiliary information into a useful prior.”</p>
</blockquote>
<p>–SAS Corporation</p>
<h1 id="formal-derivation-of-bayes-theorem">Formal Derivation of Bayes Theorem</h1>
<blockquote>
<p>Following inspiration from Kruschke (2011).</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Probability</th>
<th style="text-align: right;">A</th>
<th style="text-align: right;">Not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">B</td>
<td style="text-align: right;"><span class="math inline">\(P_1\)</span></td>
<td style="text-align: right;"><span class="math inline">\(P_2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Not B</td>
<td style="text-align: right;"><span class="math inline">\(P_3\)</span></td>
<td style="text-align: right;"><span class="math inline">\(P_4\)</span></td>
</tr>
</tbody>
</table>
<p>Filling in the probabilities.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Probability</th>
<th style="text-align: left;">A</th>
<th style="text-align: left;">Not A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">B</td>
<td style="text-align: left;"><span class="math inline">\(P(A, B)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(P(\text{not} A, B)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Not B</td>
<td style="text-align: left;"><span class="math inline">\(P(A, \text{not} B)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(P(\text{not} A, \text{not} B)\)</span></td>
</tr>
</tbody>
</table>
<p>From the definition of conditional probability:</p>
<p><span class="math inline">\(P(A|B) = P(A,B) / P(B)\)</span></p>
<p><span class="math inline">\(P(B|A) = P(A,B) / P(A)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(A|B)P(B) = P(A,B)\)</span></p>
<p><span class="math inline">\(P(B|A)P(A) = P(A,B)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(A|B)P(B) = P(B|A)P(A)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<h1 id="applying-the-derivation-to-data-analysis">Applying the Derivation to Data Analysis</h1>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Probability</th>
<th style="text-align: left;">Data</th>
<th style="text-align: left;">Not Data</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hypothesis</td>
<td style="text-align: left;"><span class="math inline">\(P(D, H)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(P(\text{not} D, H)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Not Hypothesis</td>
<td style="text-align: left;"><span class="math inline">\(P(D, \text{not} H)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(P(\text{not} D, \text{not} H)\)</span></td>
</tr>
</tbody>
</table>
<p>From the definition of conditional probability:</p>
<p><span class="math inline">\(P(D|H) = P(D,H) / P(H)\)</span></p>
<p><span class="math inline">\(P(H|D) = P(D,H) / P(D)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(D|H)P(H) = P(D,H)\)</span></p>
<p><span class="math inline">\(P(H|D)P(D) = P(D,H)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(D|H)P(H) = P(H|D)P(D)\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(P(H|D) = \frac{P(D|H)P(H)}{P(D)}\)</span></p>
<p><span class="math inline">\(\text{posterior} \sim \text{likelihood} \times \text{prior}\)</span></p>
<h1 id="accepting-the-null-hypothesis">Accepting the Null Hypothesis</h1>
<h2 id="we-are-directly-estimating-the-probability-of-the-hypothesis-given-the-data">We Are Directly Estimating The Probability of the Hypothesis Given The Data</h2>
<ul>
<li>Could be large e.g. .8.</li>
<li>Could be small e.g. .1.</li>
<li>Could be effectively 0. (<em>Essentially, we can accept a null hypothesis</em>)</li>
</ul>
<h2 id="we-are-not-rejecting-a-null-hypothesis">We Are <em>Not</em> Rejecting a Null Hypothesis</h2>
<p>We are <em>not</em> imagining a hypothetical <em>null hypothesis</em> (<em>that may not even be substantively meaningful</em>), and asking the question of whether the data we observe are extreme enough that we wish to reject this null hypothesis.</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\bar{x} = 0\)</span> or <span class="math inline">\(\beta = 0\)</span></li>
<li>Posit <span class="math inline">\(H_A\)</span>: <span class="math inline">\(\bar{x} \neq 0\)</span> or <span class="math inline">\(\beta \neq 0\)</span></li>
<li>Observe data and calculate a test statistic (e.g. <span class="math inline">\(t\)</span>). If <span class="math inline">\(\text{test statistic} &gt; \text{critical value}\)</span>, e.g. <span class="math inline">\(t &gt; 1.96\)</span> then reject <span class="math inline">\(H_0\)</span>.</li>
<li>We can never <em>accept</em> <span class="math inline">\(H_0\)</span>, only <em>reject</em> <span class="math inline">\(H_A\)</span>.</li>
</ul>
<h2 id="accepting-null-hypotheses">Accepting Null Hypotheses</h2>
<p>What is the effect on science and publication of having a statistical practice where we can never affirm <span class="math inline">\(\bar{x} = 0\)</span> or <span class="math inline">\(\beta = 0\)</span>, but only reject <span class="math inline">\(\bar{x} = 0\)</span> or <span class="math inline">\(\beta = 0\)</span>?</p>
<ul>
<li>Only affirm difference not similarity</li>
<li>Publication bias</li>
</ul>
<blockquote>
<p>See <a href="https://agrogan1.github.io/Bayes/accepting-H0/accepting-H0.html">https://agrogan1.github.io/Bayes/accepting-H0/accepting-H0.html</a></p>
</blockquote>
<blockquote>
<p>Bayesian statistics allow us to accept the null hypothesis <span class="math inline">\(H_0\)</span>.</p>
</blockquote>
<h1 id="bayesian-categorical-data-analysis-in-stata">Bayesian Categorical Data Analysis in Stata</h1>
<pre class='stata'>. clear all
</pre>
<pre class='stata'>. set seed 1234 // setting random seed is important!!!
</pre>
<pre class='stata'>. use "../logistic-regression/GSSsmall.dta", clear
</pre>
<h2 id="frequentist-logistic-regression">Frequentist Logistic Regression</h2>
<pre class='stata'>. logit liberal i.race i.class

Iteration 0:   log likelihood = -31538.733  
Iteration 1:   log likelihood = -31370.507  
Iteration 2:   log likelihood = -31369.841  
Iteration 3:   log likelihood = -31369.841  

Logistic regression                             Number of obs     =     53,625
                                                LR chi2(5)        =     337.78
                                                Prob > chi2       =     0.0000
Log likelihood = -31369.841                     Pseudo R2         =     0.0054

───────────────┬────────────────────────────────────────────────────────────────
       liberal │      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
───────────────┼────────────────────────────────────────────────────────────────
          race │
        black  │   .4443531   .0272062    16.33   0.000       .39103    .4976762
        other  │   .3190896   .0413275     7.72   0.000     .2380891    .4000901
               │
         class │
working class  │  -.1397848    .041515    -3.37   0.001    -.2211527   -.0584169
 middle class  │  -.0117948   .0416509    -0.28   0.777     -.093429    .0698394
  upper class  │   .1512565   .0648962     2.33   0.020     .0240624    .2784507
               │
         _cons │  -.9900441   .0397384   -24.91   0.000     -1.06793   -.9121582
───────────────┴────────────────────────────────────────────────────────────────
</pre>
<h2 id="bayesian-logistic-regression">Bayesian Logistic Regression</h2>
<blockquote>
<p>Takes a few minutes since using MCMC (5-10 minutes).</p>
</blockquote>
<pre class='stata'>. sample 10 // Random Sample To Speed This Example: DON'T DO THIS IN PRACTICE!!!
(58,332 observations deleted)
</pre>
<blockquote>
<p>How do we interpret the result for some of the <strong>social class</strong> categories where the credibility interval includes 0?</p>
</blockquote>
<pre class='stata'>. bayes: logit liberal i.race i.class
  
Burn-in ...
Simulation ...

Model summary
────────────────────────────────────────────────────────────────────────────────
Likelihood: 
  liberal ~ logit(xb_liberal)

Prior: 
  {liberal:i.race i.class _cons} ~ normal(0,10000)                           (1)
────────────────────────────────────────────────────────────────────────────────
(1) Parameters are elements of the linear form xb_liberal.

Bayesian logistic regression                       MCMC iterations  =     12,500
Random-walk Metropolis-Hastings sampling           Burn-in          =      2,500
                                                   MCMC sample size =     10,000
                                                   Number of obs    =      5,376
                                                   Acceptance rate  =      .2312
                                                   Efficiency:  min =     .01541
                                                                avg =     .03017
Log marginal likelihood = -3193.2465                            max =     .05577
 
───────────────┬────────────────────────────────────────────────────────────────
               │                                                Equal-tailed
       liberal │      Mean   Std. Dev.     MCSE     Median  [95% Cred. Interval]
───────────────┼────────────────────────────────────────────────────────────────
          race │
        black  │  .5186618   .0888498   .005436   .5162073   .3446927   .6905559
        other  │  .3315087   .1318099   .006538   .3340969   .0778656   .5812581
               │
         class │
working class  │ -.2257059   .1359429    .01095  -.2304211  -.4719162   .0560403
 middle class  │ -.2159555   .1280385   .008659  -.2177452  -.4572864   .0353198
  upper class  │  .1385091   .2119785   .008976   .1421824  -.2664372   .5469788
               │
         _cons │ -.8561818   .1277022   .008896  -.8537522  -1.104622  -.6151415
───────────────┴────────────────────────────────────────────────────────────────
Note: Default priors are used for model parameters.
</pre>
<h2 id="blocking-may-improve-estimation">Blocking May Improve Estimation</h2>
<pre class='stata'>. * bayes, block({liberal:i.race}): logit liberal i.race i.class // blocking may improve estimatio
> n
</pre>
<h2 id="bayesian-logistic-regression-with-priors">Bayesian Logistic Regression With Priors</h2>
<p>Priors:</p>
<ul>
<li>Encode prior information: strong theory; strong clinical or practice wisdom; strong previous empirical results.</li>
<li>May be helpful in quantitatively encoding the results of prior literature.</li>
<li>May be especially helpful when your sample is small.</li>
</ul>
<pre class='stata'>. bayes, normalprior(5): logit liberal i.race i.class
  
Burn-in ...
Simulation ...

Model summary
────────────────────────────────────────────────────────────────────────────────
Likelihood: 
  liberal ~ logit(xb_liberal)

Prior: 
  {liberal:i.race i.class _cons} ~ normal(0,25)                              (1)
────────────────────────────────────────────────────────────────────────────────
(1) Parameters are elements of the linear form xb_liberal.

Bayesian logistic regression                       MCMC iterations  =     12,500
Random-walk Metropolis-Hastings sampling           Burn-in          =      2,500
                                                   MCMC sample size =     10,000
                                                   Number of obs    =      5,376
                                                   Acceptance rate  =      .2296
                                                   Efficiency:  min =     .02373
                                                                avg =     .03808
Log marginal likelihood = -3175.5153                            max =     .05215
 
───────────────┬────────────────────────────────────────────────────────────────
               │                                                Equal-tailed
       liberal │      Mean   Std. Dev.     MCSE     Median  [95% Cred. Interval]
───────────────┼────────────────────────────────────────────────────────────────
          race │
        black  │  .5156108   .0846052   .003705   .5165275   .3428716   .6703037
        other  │  .3494915   .1292596   .007216   .3517041   .0891921   .6044571
               │
         class │
working class  │ -.2177134   .1271378   .005941  -.2191734  -.4736636   .0299706
 middle class  │ -.2111361   .1279262   .006815   -.209842  -.4649101   .0467745
  upper class  │  .1408649   .2085374   .013539   .1413301  -.2595456   .5542024
               │
         _cons │ -.8599554   .1222741   .006154  -.8616087  -1.102605   -.620957
───────────────┴────────────────────────────────────────────────────────────────
Note: Default priors are used for model parameters.
</pre>
<h2 id="mcmc-vs.-ml">MCMC vs. ML</h2>
<pre class='stata'>. clear all
</pre>
<pre class='stata'>. set obs 100
number of observations (_N) was 0, now 100
</pre>
<pre class='stata'>. generate myestimate = rnormal() + 10 // simulated values of estimate
</pre>
<pre class='stata'>. summarize myestimate

    Variable │        Obs        Mean    Std. Dev.       Min        Max
─────────────┼─────────────────────────────────────────────────────────
  myestimate │        100     9.94191    .9294447   7.932717   12.31949
</pre>
<pre class='stata'>. local mymean = r(mean)
</pre>
<pre class='stata'>. kdensity myestimate ,  ///
> title("Likelihood of Estimate") ///
> xtitle("Estimate") ytitle("Likelihood") ///
> note("Vertical Line At Mean Value") ///
> caption("ML gives point estimate; Bayes gives full range of distribution") ///
> xline(`mymean', lwidth(1) lcolor(gold)) scheme(michigan)
</pre>
<pre class='stata'>. graph export MCMC-ML.png, width(500) replace
(file MCMC-ML.png written in PNG format)
</pre>
<figure>
<img src="MCMC-ML.png" alt="" /><figcaption>MCMC vs. ML</figcaption>
</figure>
<h2 id="full-distribution-of-parameters">Full Distribution of Parameters</h2>
<pre class='stata'>. clear all
</pre>
<pre class='stata'>. use "../logistic-regression/GSSsmall.dta", clear
</pre>
<pre class='stata'>. sample 10 // Random Sample for These Slides: DON'T DO THIS IN PRACTICE!!!
(58,332 observations deleted)
</pre>
<pre class='stata'>. bayes, normalprior(5): logit liberal i.race i.class
  
Burn-in ...
Simulation ...

Model summary
────────────────────────────────────────────────────────────────────────────────
Likelihood: 
  liberal ~ logit(xb_liberal)

Prior: 
  {liberal:i.race i.class _cons} ~ normal(0,25)                              (1)
────────────────────────────────────────────────────────────────────────────────
(1) Parameters are elements of the linear form xb_liberal.

Bayesian logistic regression                       MCMC iterations  =     12,500
Random-walk Metropolis-Hastings sampling           Burn-in          =      2,500
                                                   MCMC sample size =     10,000
                                                   Number of obs    =      5,383
                                                   Acceptance rate  =      .2236
                                                   Efficiency:  min =     .02256
                                                                avg =     .03414
Log marginal likelihood = -3177.2034                            max =     .05443
 
───────────────┬────────────────────────────────────────────────────────────────
               │                                                Equal-tailed
       liberal │      Mean   Std. Dev.     MCSE     Median  [95% Cred. Interval]
───────────────┼────────────────────────────────────────────────────────────────
          race │
        black  │  .4851672   .0829121   .004159   .4879013   .3172142   .6439872
        other  │  .0424599    .135287   .005799   .0432961   -.223915   .3134179
               │
         class │
working class  │ -.3129757   .1321655     .0088  -.3171116  -.5767932  -.0470307
 middle class  │ -.2267685   .1281627   .008449  -.2287587  -.4673167   .0249752
  upper class  │ -.1154092   .2013339   .010816  -.1178767  -.5131633   .2788442
               │
         _cons │ -.7892161   .1229919   .007051  -.7913504  -1.037607  -.5534833
───────────────┴────────────────────────────────────────────────────────────────
Note: Default priors are used for model parameters.
</pre>
<pre class='stata'>. bayesgraph kdensity {liberal:2.race}, scheme(michigan)
</pre>
<pre class='stata'>. graph export mybayesgraph.png, width(500) replace
(file mybayesgraph.png written in PNG format)
</pre>
<figure>
<img src="mybayesgraph.png" alt="" /><figcaption>Density Plot of Parameter</figcaption>
</figure>
</body>
</html>
